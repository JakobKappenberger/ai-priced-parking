{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "\n",
    "import pyNetLogo\n",
    "\n",
    "#netlogo = pyNetLogo.NetLogoLink(gui=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netlogo.load_model('.Social_Simulation_Seminar_Model.nlogo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netlogo.command('setup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "netlogo.repeat_command(\"go\", 100)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netlogo.kill_workspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coor = netlogo.patch_report('fee')\n",
    "coor = coor.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coor[np.where(coor > 0)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netlogo.report('num-cars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = netlogo.patch_report('pcolor')\n",
    "fees = netlogo.patch_report('fee')\n",
    "lot_colors = netlogo.report(\"lot-colors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lot_masks = dict()\n",
    "for i, c in enumerate(['yellow', 'orange', 'green', 'blue']):\n",
    "    lot_masks[c] = colors == lot_colors[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 91)\n",
    "pd.set_option('display.max_columns', 91)\n",
    "fees[lot_masks['yellow']]= fees[lot_masks['yellow']] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netlogo.patch_set(\"fee\", fees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_resolution = netlogo.report(\"temporal-resolution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 91)\n",
    "pd.set_option('display.max_columns', 91)\n",
    "\n",
    "fees[colors==lot_colors[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(fees[colors==lot_color[1]].to_numpy()[~np.isnan(fees[colors==lot_color[1]].to_numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fees[colors==lot_color[1]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netlogo.patch_set(\"pcolor\", colors.mask(colors==lot_color[0], lot_color[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coor.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorforce.environments import Environment\n",
    "\n",
    "\n",
    "class CustomEnvironment(Environment):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.finished = False\n",
    "        self.episode_end = False\n",
    "        self.netlogo = pyNetLogo.NetLogoLink(gui=True)\n",
    "        self.netlogo.load_model('.Social_Simulation_Seminar_Model.nlogo')\n",
    "        self.netlogo.command('setup')\n",
    "        lot_colors = self.netlogo.report(\"lot-colors\")\n",
    "        patch_colors = self.netlogo.patch_report('pcolor')\n",
    "        # Create dict with boolean masks for different lots\n",
    "        self.lot_masks = dict()\n",
    "        for i, c in enumerate(['yellow', 'orange', 'green', 'blue']):\n",
    "            self.lot_masks[c] = patch_colors == lot_colors[i]\n",
    "        pd.set_option('display.max_rows', 91)\n",
    "        pd.set_option('display.max_columns', 91)\n",
    "        self.temporal_resolution = self.netlogo.report(\"temporal-resolution\")\n",
    "        self.n_cars = float(self.netlogo.report(\"n-cars\"))\n",
    "\n",
    "    def states(self):\n",
    "        return dict(type=\"float\", shape=(5,))\n",
    "\n",
    "    def actions(self):\n",
    "        return {\n",
    "            \"yellow\": dict(type=\"int\", num_values=3),\n",
    "            \"orange\": dict(type=\"int\", num_values=3),\n",
    "            \"green\": dict(type=\"int\", num_values=3),\n",
    "            \"blue\": dict(type=\"int\", num_values=3)\n",
    "        }\n",
    "\n",
    "    # Optional: should only be defined if environment has a natural fixed\n",
    "    # maximum episode length; otherwise specify maximum number of training\n",
    "    # timesteps via Environment.create(..., max_episode_timesteps=???)\n",
    "    def max_episode_timesteps(self):\n",
    "        return super().max_episode_timesteps()\n",
    "\n",
    "    # Optional additional steps to close environment\n",
    "    def close(self):\n",
    "        self.netlogo.kill_workspace()\n",
    "        super().close()\n",
    "\n",
    "    def reset(self):\n",
    "        self.netlogo.command('setup')\n",
    "        state = self.get_state()\n",
    "        return state\n",
    "\n",
    "    def execute(self, actions):\n",
    "        next_state = self.compute_step(actions)\n",
    "        terminal = self.terminal()\n",
    "        reward = self.reward()\n",
    "        return next_state, terminal, reward\n",
    "    \n",
    "    def compute_step(self, actions):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Move simulation forward\n",
    "        self.netlogo.repeat_command(\"go\", self.temporal_resolution / 4)\n",
    "        \n",
    "        # Adjust prices and query state\n",
    "        new_state = self.adjust_prices(actions)\n",
    "        \n",
    "        print(new_state)\n",
    "        \n",
    "        return new_state\n",
    "            \n",
    "    def adjust_prices(self, actions):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        print(actions)\n",
    "        fees = self.netlogo.patch_report('fee')\n",
    "        for c in actions.keys():\n",
    "            c_action = actions[c]\n",
    "            if c_action == 0:\n",
    "                self.netlogo.command(f\"ask {c}-lot [set fee fee - 0.5]\")\n",
    "            elif c_action == 1:\n",
    "                continue\n",
    "            elif c_action == 2:\n",
    "                self.netlogo.command(f\"ask {c}-lot [set fee fee + 0.5]\")\n",
    "                \n",
    "        #self.netlogo.patch_set(\"fee\", fees)\n",
    "\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        state = []\n",
    "        self.n_cars = float(self.netlogo.report(\"n-cars\"))\n",
    "        state.append(self.n_cars)\n",
    "        fees = self.netlogo.patch_report('fee')\n",
    "        for c in ['yellow', 'orange', 'green', 'blue']:\n",
    "            lot_fee = fees[self.lot_masks[c]].to_numpy()\n",
    "            state.append(lot_fee[(~np.isnan(lot_fee))& (lot_fee > 0)][0])\n",
    "            \n",
    "        return state\n",
    "    \n",
    "\n",
    "    def terminal(self):\n",
    "        self.finished = self.n_cars < 300\n",
    "        return self.finished\n",
    "\n",
    "    def reward(self):\n",
    "        if self.finished:\n",
    "            reward = 500\n",
    "        else:\n",
    "            reward = -1\n",
    "        return reward\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment.create(\n",
    "    environment=CustomEnvironment, max_episode_timesteps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorforce.agents import Agent\n",
    "agent = Agent.create(\n",
    "    agent='ppo', environment=environment, batch_size=10, learning_rate=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 100 episodes\n",
    "for _ in range(100):\n",
    "    states = environment.reset()\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        actions = agent.act(states=states)\n",
    "        states, terminal, reward = environment.execute(actions=actions)\n",
    "        agent.observe(terminal=terminal, reward=reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyNetLogo\n",
    "from tensorforce.environments import Environment\n",
    "from tensorforce.execution import Runner\n",
    "\n",
    "from util import occupancy_reward_function\n",
    "\n",
    "COLOURS = ['yellow', 'orange', 'green', 'blue']\n",
    "TIMESTAMP = datetime.now().strftime('%y%m-%d%H-%M')\n",
    "\n",
    "\n",
    "class CustomEnvironment(Environment):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.path = \"./Experiments/\" + TIMESTAMP\n",
    "        self.finished = False\n",
    "        self.episode_end = False\n",
    "        # Episode Counter\n",
    "        self.episode = 1\n",
    "        self.nl = pyNetLogo.NetLogoLink(gui=True)\n",
    "        self.nl.load_model('.Social_Simulation_Seminar_Model.nlogo')\n",
    "        self.nl.command('setup')\n",
    "        # Disable rendering of view\n",
    "        self.nl.command('no-display')\n",
    "        # Turn baseline pricing mechanism off\n",
    "        self.nl.command('set dynamic-pricing-baseline false')\n",
    "        # Record data\n",
    "        self.nl.command(\"ask one-of cars [record-data]\")\n",
    "        # Save current state in dict\n",
    "        self.current_state = dict()\n",
    "        self.current_state['ticks'] = self.nl.report(\"ticks\")\n",
    "        self.current_state['n_cars'] = float(self.nl.report(\"n-cars\"))\n",
    "        self.current_state['overall_occupancy'] = self.nl.report(\"global-occupancy\")\n",
    "\n",
    "        # General information about model\n",
    "        self.temporal_resolution = self.nl.report(\"temporal-resolution\")\n",
    "        self.n_garages = self.nl.report(\"num-garages\")\n",
    "        self.colours = COLOURS\n",
    "\n",
    "    def states(self):\n",
    "        if self.n_garages > 0:\n",
    "            return dict(type=\"float\", shape=(12,))\n",
    "        else:\n",
    "            return dict(type=\"float\", shape=(11,))\n",
    "\n",
    "    def actions(self):\n",
    "        return {\n",
    "            \"yellow\": dict(type=\"int\", num_values=3),\n",
    "            \"orange\": dict(type=\"int\", num_values=3),\n",
    "            \"green\": dict(type=\"int\", num_values=3),\n",
    "            \"blue\": dict(type=\"int\", num_values=3)\n",
    "        }\n",
    "\n",
    "    # Optional: should only be defined if environment has a natural fixed\n",
    "    # maximum episode length; otherwise specify maximum number of training\n",
    "    # timesteps via Environment.create(..., max_episode_timesteps=???)\n",
    "    def max_episode_timesteps(self):\n",
    "        return super().max_episode_timesteps()\n",
    "\n",
    "    # Optional additional steps to close environment\n",
    "    def close(self):\n",
    "        self.nl.kill_workspace()\n",
    "        super().close()\n",
    "\n",
    "    def reset(self):\n",
    "        self.nl.command('setup')\n",
    "        state = self.get_state()\n",
    "        return state\n",
    "\n",
    "    def execute(self, actions):\n",
    "        next_state = self.compute_step(actions)\n",
    "        terminal = self.terminal()\n",
    "        reward = self.reward()\n",
    "        if terminal:\n",
    "            self.document_episode()\n",
    "        return next_state, terminal, reward\n",
    "\n",
    "    def compute_step(self, actions):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Move simulation forward\n",
    "        self.nl.repeat_command(\"go\", self.temporal_resolution / 2)\n",
    "\n",
    "        # Adjust prices and query state\n",
    "        new_state = self.adjust_prices(actions)\n",
    "\n",
    "        return new_state\n",
    "\n",
    "    def adjust_prices(self, actions):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # print(actions)\n",
    "        for c in actions.keys():\n",
    "            c_action = actions[c]\n",
    "            if c_action == 0:\n",
    "                self.nl.command(f\"change-fee {c}-lot -0.5\")\n",
    "            elif c_action == 1:\n",
    "                continue\n",
    "            elif c_action == 2:\n",
    "                self.nl.command(f\"change-fee {c}-lot 0.5\")\n",
    "\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Update view in NetLogo once\n",
    "        self.nl.command('display')\n",
    "        self.nl.command('no-display')\n",
    "        # Update globals\n",
    "        self.nl.command(\"ask one-of cars [record-data]\")\n",
    "        self.current_state['ticks'] = self.nl.report(\"ticks\")\n",
    "        self.current_state['n_cars'] = float(self.nl.report(\"n-cars\"))\n",
    "        self.current_state['overall_occupancy'] = self.nl.report(\"global-occupancy\")\n",
    "\n",
    "        # Append fees and current occupation to state\n",
    "        for c in self.colours:\n",
    "            self.current_state[f'{c}-lot fee'] = self.nl.report(f\"{c}-lot-current-fee\")\n",
    "            self.current_state[f'{c}-lot occupancy'] = self.nl.report(f\"{c}-lot-current-occup\")\n",
    "\n",
    "        if self.n_garages > 0:\n",
    "            self.current_state['garages occupancy'] = self.nl.report(\"garages-current-occup\")\n",
    "\n",
    "        state = list(self.current_state.values())\n",
    "        return state\n",
    "\n",
    "    def terminal(self):\n",
    "        self.episode_end = self.current_state['ticks'] >= self.temporal_resolution * 12\n",
    "        self.finished = self.current_state['n_cars'] < 100\n",
    "\n",
    "        return self.finished or self.episode_end\n",
    "\n",
    "    def reward(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        return occupancy_reward_function(self)\n",
    "\n",
    "    def document_episode(self):\n",
    "        \"\"\"\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Path(self.path).mkdir(parents=True, exist_ok=True)\n",
    "        # # Get all directories to check, which Episode this is\n",
    "        # dirs = glob(self.path + \"/*\")\n",
    "        # current_episode = 1\n",
    "        # if dirs:\n",
    "        #     last_episode = max([int(re.findall(\"E(\\d+)\", dirs[i])[0]) for i in range(len(dirs))])\n",
    "        #     print(last_episode)\n",
    "        #     current_episode = last_episode + 1\n",
    "        # episode_path = self.path + f\"/E{current_episode}\"\n",
    "        # Path(episode_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Check if directory exists\n",
    "        Path(self.path).mkdir(parents=True, exist_ok=True)\n",
    "        if self.episode > 1:\n",
    "            self.episode += self.episode\n",
    "        episode_path = self.path + f\"/E{self.episode}\"\n",
    "        Path(episode_path).mkdir(parents=True, exist_ok=True)\n",
    "        self.nl.command(f'export-world \"{episode_path}/nl_model.csv\"')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No min_value bound specified for state.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2eb5d3cd2cd44a19ceb980ffb18cbe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episodes:   0%|          | 0/50 [00:00, reward=0.00, ts/ep=0, sec/ep=0.00, ms/ts=0.0, agent=0.0%, comm=0.0%]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent_dict = {\n",
    "        \"agent\": \"ppo\",\n",
    "        \"optimizer\": {\n",
    "            \"learning_rate\": 1e-3\n",
    "        },\n",
    "        \"max_episode_timesteps\": 100,\n",
    "        \"batch-size\": 10\n",
    "    }\n",
    "\n",
    "# create and train the agent\n",
    "runner = Runner(agent='agent.json', environment=\"environment.CustomEnvironment\", max_episode_timesteps=500,\n",
    "                remote=\"multiprocessing\", num_parallel=4)\n",
    "runner.run(num_episodes=50)\n",
    "\n",
    "# accesing the metrics from runner\n",
    "rewards = np.asarray(runner.episode_rewards)\n",
    "episode_length = np.asarray(runner.episode_timesteps)\n",
    "\n",
    "# calculating the mean-reward per episode\n",
    "mean_reward = rewards / episode_length\n",
    "num_episodes = len(mean_reward)\n",
    "\n",
    "# plotting mean-reward over episodes\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(20, 10))\n",
    "ax1.plot(range(num_episodes), mean_reward, linewidth=3)\n",
    "# plt.xticks(fontsize=15)\n",
    "ax1.set_ylabel('mean-reward', fontsize=22)\n",
    "ax1.grid(True)\n",
    "ax1.tick_params(axis=\"y\", labelsize=15)\n",
    "# plotting episode length over episodes\n",
    "ax2.plot(range(num_episodes), rewards, linewidth=3)\n",
    "ax2.set_xlabel('# episodes', fontsize=22)\n",
    "ax2.set_ylabel('Reward', fontsize=22)\n",
    "ax2.tick_params(axis=\"y\", labelsize=15)\n",
    "ax2.tick_params(axis=\"x\", labelsize=15)\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('number of episodes during training: ', len(rewards))\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
